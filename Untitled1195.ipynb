{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f96dd9-bd6c-4f01-9722-df87f179cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded as CSV enc='utf-8', sep=',', shape=(6123270, 35)\n",
      "[INFO] n_students=29018, n_skills=265, rows=2711813\n",
      "[INFO] X_train=(24031, 200)\n",
      "[INFO] X_val=(5141, 200)\n",
      "[INFO] Saved preprocessor.pkl and label_encoder.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,984</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                               │                           │                 │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                 │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │          \u001b[38;5;34m33,984\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │          \u001b[38;5;34m98,816\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                               │                           │                 │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                 │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m1\u001b[0m)            │             \u001b[38;5;34m129\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">132,929</span> (519.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m132,929\u001b[0m (519.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">132,929</span> (519.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m132,929\u001b[0m (519.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 167ms/step - accuracy: 0.3467 - loss: 0.5873 - val_accuracy: 0.3579 - val_loss: 0.5660\n",
      "Epoch 2/30\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 165ms/step - accuracy: 0.3685 - loss: 0.5673 - val_accuracy: 0.3592 - val_loss: 0.5637\n",
      "Epoch 3/30\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 167ms/step - accuracy: 0.3715 - loss: 0.5650 - val_accuracy: 0.3647 - val_loss: 0.5615\n",
      "Epoch 4/30\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 169ms/step - accuracy: 0.3725 - loss: 0.5629 - val_accuracy: 0.3640 - val_loss: 0.5604\n",
      "Epoch 5/30\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 169ms/step - accuracy: 0.3740 - loss: 0.5619 - val_accuracy: 0.3615 - val_loss: 0.5601\n",
      "Epoch 6/30\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 172ms/step - accuracy: 0.3746 - loss: 0.5611 - val_accuracy: 0.3605 - val_loss: 0.5601\n",
      "Epoch 7/30\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 170ms/step - accuracy: 0.3747 - loss: 0.5605 - val_accuracy: 0.3614 - val_loss: 0.5598\n",
      "Epoch 8/30\n",
      "\u001b[1m376/376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 171ms/step - accuracy: 0.3749 - loss: 0.5600 - val_accuracy: 0.3591 - val_loss: 0.5597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved model.h5 -> C:\\Users\\sagni\\Downloads\\SkillTracer Knowledge Tracing\\model.h5\n",
      "[INFO] Saved model_config.yaml -> C:\\Users\\sagni\\Downloads\\SkillTracer Knowledge Tracing\n"
     ]
    }
   ],
   "source": [
    "import os, csv, json, pickle, warnings, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "DATA_PATH = r\"C:\\Users\\sagni\\Downloads\\SkillTracer Knowledge Tracing\\archive (1)\\2012-2013-data-with-predictions-4-final.csv\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\sagni\\Downloads\\SkillTracer Knowledge Tracing\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Reproducibility\n",
    "# --------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# --------------------------\n",
    "# Robust reader (CSV or Excel-in-disguise)\n",
    "# --------------------------\n",
    "def is_zip_or_xlsx(path):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return f.read(2) == b\"PK\"\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def robust_read_any(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    if is_zip_or_xlsx(path):\n",
    "        import openpyxl\n",
    "        df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "        print(f\"[INFO] Loaded as Excel: {os.path.basename(path)} shape={df.shape}\")\n",
    "        return df\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"]\n",
    "    delimiters = [\";\", \",\", \"\\t\", \"|\"]\n",
    "    # sniff delimiter quickly\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            head = f.read(8192).decode(\"latin1\", errors=\"ignore\")\n",
    "        try:\n",
    "            sn = csv.Sniffer().sniff(head)\n",
    "            if sn.delimiter in delimiters:\n",
    "                delimiters = [sn.delimiter] + [d for d in delimiters if d != sn.delimiter]\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        for sep in delimiters:\n",
    "            try:\n",
    "                df = pd.read_csv(path, encoding=enc, sep=sep, engine=\"python\")\n",
    "                if df.shape[1] > 1:\n",
    "                    print(f\"[INFO] Loaded as CSV enc='{enc}', sep='{sep}', shape={df.shape}\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "    raise RuntimeError(f\"Could not parse {path}. Last error: {last_err}\")\n",
    "\n",
    "df = robust_read_any(DATA_PATH)\n",
    "\n",
    "# --------------------------\n",
    "# Column mapping (auto-detect common names)\n",
    "# --------------------------\n",
    "def pick_col(candidates, cols):\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    lc = {c.lower(): c for c in cols}\n",
    "    for c in candidates:\n",
    "        if c.lower() in lc:\n",
    "            return lc[c.lower()]\n",
    "    return None\n",
    "\n",
    "cols = list(df.columns)\n",
    "student_col = pick_col([\"student_id\",\"user_id\",\"Anon Student Id\",\"Anon StudentID\",\"student\",\"sid\"], cols)\n",
    "skill_col   = pick_col([\"skill_id\",\"skill\",\"tag\",\"KC(SubSkills)\",\"KC\",\"skill_name\",\"concept_id\"], cols)\n",
    "correct_col = pick_col([\"correct\",\"is_correct\",\"Correct First Attempt\",\"answered_correctly\",\"label\"], cols)\n",
    "time_col    = pick_col([\"timestamp\",\"start_time\",\"order_id\",\"time\",\"Time\"], cols)\n",
    "\n",
    "if student_col is None or skill_col is None or correct_col is None:\n",
    "    raise ValueError(f\"Missing key columns in {cols} — need student_id-like, skill_id/tag-like, and correct-like columns.\")\n",
    "\n",
    "# Normalize correctness to {0,1}\n",
    "df[correct_col] = pd.to_numeric(df[correct_col], errors=\"coerce\")\n",
    "df[correct_col] = (df[correct_col] > 0).astype(int)\n",
    "\n",
    "# If multiple skills per row, take the first\n",
    "def take_first_skill(v):\n",
    "    if pd.isna(v): return np.nan\n",
    "    s = str(v)\n",
    "    for sep in [\"~~\",\"; \", \";\", \",\", \"|\"]:\n",
    "        if sep in s:\n",
    "            return s.split(sep)[0]\n",
    "    return s\n",
    "\n",
    "df[skill_col] = df[skill_col].apply(take_first_skill)\n",
    "\n",
    "# Sort per student by time (if available)\n",
    "if time_col is not None:\n",
    "    try:\n",
    "        df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
    "        df = df.sort_values([student_col, time_col]).reset_index(drop=True)\n",
    "    except Exception:\n",
    "        df = df.sort_values([student_col]).reset_index(drop=True)\n",
    "else:\n",
    "    df = df.sort_values([student_col]).reset_index(drop=True)\n",
    "\n",
    "# Drop missing essentials\n",
    "df = df.dropna(subset=[student_col, skill_col, correct_col])\n",
    "\n",
    "# --------------------------\n",
    "# Build integer encoders & save as preprocessor.pkl\n",
    "# --------------------------\n",
    "students = df[student_col].astype(str).unique().tolist()\n",
    "skills   = df[skill_col].astype(str).unique().tolist()\n",
    "\n",
    "student2idx = {s:i for i,s in enumerate(students)}\n",
    "idx2student = {i:s for s,i in student2idx.items()}\n",
    "skill2idx   = {s:i for i,s in enumerate(skills)}\n",
    "idx2skill   = {i:s for s,i in skill2idx.items()}\n",
    "\n",
    "df[\"_sid\"]  = df[student_col].astype(str).map(student2idx)\n",
    "df[\"_kid\"]  = df[skill_col].astype(str).map(skill2idx)\n",
    "df[\"_corr\"] = df[correct_col].astype(int)\n",
    "\n",
    "n_skills = len(skill2idx)\n",
    "print(f\"[INFO] n_students={len(student2idx)}, n_skills={n_skills}, rows={len(df)}\")\n",
    "\n",
    "# --------------------------\n",
    "# Build sequences per student for DKT\n",
    "# token = 1 + skill_id + correctness * n_skills (vocab: 1..2*n_skills; 0 is PAD)\n",
    "# Predict next correctness at each timestep\n",
    "# --------------------------\n",
    "MAX_LEN = 200\n",
    "\n",
    "def build_sequences_for_student(sub):\n",
    "    kid = sub[\"_kid\"].values.astype(np.int32)\n",
    "    corr = sub[\"_corr\"].values.astype(np.int32)\n",
    "    if len(kid) < 2:  # need at least 2 interactions to create (x->y)\n",
    "        return []\n",
    "    tokens = (kid[:-1] + corr[:-1]*n_skills + 1).astype(np.int32)\n",
    "    y      = corr[1:].astype(np.int32)\n",
    "    chunks = []\n",
    "    for start in range(0, len(tokens), MAX_LEN):\n",
    "        xt = tokens[start:start+MAX_LEN]\n",
    "        yt = y[start:start+MAX_LEN]\n",
    "        if len(xt) == 0: \n",
    "            continue\n",
    "        pad = MAX_LEN - len(xt)\n",
    "        xt = np.pad(xt, (0,pad), constant_values=0)              # (T,)\n",
    "        yt = np.pad(yt, (0,pad), constant_values=-1).astype(int) # (T,) with -1 as pad label\n",
    "        mask = (yt != -1).astype(np.float32)                     # (T,)\n",
    "        yt = np.where(yt==-1, 0, yt).astype(np.float32)          # replace pad with 0 for loss\n",
    "        chunks.append((xt, yt, mask))\n",
    "    return chunks\n",
    "\n",
    "# Split by student (to avoid leakage)\n",
    "sid_all = df[\"_sid\"].unique()\n",
    "train_sids, test_sids = train_test_split(sid_all, test_size=0.15, random_state=SEED)\n",
    "train_sids, val_sids  = train_test_split(train_sids, test_size=0.1765, random_state=SEED)  # ~0.70/0.15/0.15\n",
    "\n",
    "def collect(sids):\n",
    "    X, Y, W = [], [], []\n",
    "    for sid in sids:\n",
    "        sub = df[df[\"_sid\"] == sid]\n",
    "        for xt, yt, mw in build_sequences_for_student(sub):\n",
    "            X.append(xt)\n",
    "            Y.append(yt[:, None])   # (T,1) to match Dense(1) output\n",
    "            W.append(mw)            # (T,)\n",
    "    if len(X)==0: return None, None, None\n",
    "    return np.stack(X), np.stack(Y), np.stack(W)\n",
    "\n",
    "X_train, Y_train, W_train = collect(train_sids)\n",
    "X_val,   Y_val,   W_val   = collect(val_sids)\n",
    "\n",
    "for name, arr in [(\"X_train\",X_train),(\"X_val\",X_val)]:\n",
    "    if arr is None:\n",
    "        raise RuntimeError(f\"{name} is empty — check column mapping or data quality.\")\n",
    "    print(f\"[INFO] {name}={arr.shape}\")\n",
    "\n",
    "# --------------------------\n",
    "# Save preprocessors & label encoder (.pkl)\n",
    "# --------------------------\n",
    "preproc = {\n",
    "    \"student2idx\": student2idx,\n",
    "    \"idx2student\": idx2student,\n",
    "    \"skill2idx\":   skill2idx,\n",
    "    \"idx2skill\":   idx2skill,\n",
    "    \"n_skills\":    n_skills,\n",
    "    \"max_len\":     MAX_LEN,\n",
    "    \"token_pad_id\": 0,\n",
    "    \"token_vocab_size\": 2*n_skills + 1,\n",
    "    \"token_rule\": \"token = 1 + skill_id + correctness * n_skills\",\n",
    "    \"columns\": {\n",
    "        \"student\": student_col,\n",
    "        \"skill\":   skill_col,\n",
    "        \"correct\": correct_col,\n",
    "        \"time\":    time_col\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"preprocessor.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(preproc, f)\n",
    "\n",
    "le = LabelEncoder().fit([0,1])\n",
    "with open(os.path.join(OUTPUT_DIR, \"label_encoder.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(\"[INFO] Saved preprocessor.pkl and label_encoder.pkl\")\n",
    "\n",
    "# --------------------------\n",
    "# Build & train DKT model\n",
    "# --------------------------\n",
    "VOCAB_SIZE   = preproc[\"token_vocab_size\"]\n",
    "EMBED_DIM    = 64\n",
    "HIDDEN_UNITS = 128\n",
    "\n",
    "inp = keras.Input(shape=(MAX_LEN,), dtype=\"int32\")\n",
    "x = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM, mask_zero=True)(inp)\n",
    "x = layers.LSTM(HIDDEN_UNITS, return_sequences=True)(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inp, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")])\n",
    "model.summary()\n",
    "\n",
    "early = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    sample_weight=W_train,\n",
    "    validation_data=(X_val, Y_val, W_val),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Save model (.h5) and config (.yaml or JSON fallback)\n",
    "# --------------------------\n",
    "h5_path = os.path.join(OUTPUT_DIR, \"model.h5\")\n",
    "model.save(h5_path)\n",
    "print(f\"[INFO] Saved model.h5 -> {h5_path}\")\n",
    "\n",
    "# YAML config\n",
    "saved_yaml = False\n",
    "try:\n",
    "    import yaml\n",
    "    cfg = model.get_config()\n",
    "    with open(os.path.join(OUTPUT_DIR, \"model_config.yaml\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(cfg, f, sort_keys=False)\n",
    "    saved_yaml = True\n",
    "    print(f\"[INFO] Saved model_config.yaml -> {OUTPUT_DIR}\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Could not write YAML (install pyyaml). Saving JSON instead:\", e)\n",
    "\n",
    "if not saved_yaml:\n",
    "    with open(os.path.join(OUTPUT_DIR, \"model_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(model.get_config(), f, indent=2)\n",
    "    print(f\"[INFO] Saved model_config.json -> {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e92be-160e-4d32-b8fa-c78ed61ab57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
